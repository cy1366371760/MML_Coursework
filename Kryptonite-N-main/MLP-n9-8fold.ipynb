{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load date\n",
    "X = np.load(\"Datasets/kryptonite-9-X.npy\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.load(\"Datasets/kryptonite-9-y.npy\")\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float)\n",
    "# create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# define split sizes (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# build model for training\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=128, output_size=1):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.layer3 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.layer4 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.layer5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = torch.sigmoid(self.layer5(x))  # sigmoid activation for binary output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, loss_fn, optimizer, scheduler):\n",
    "    # set the number of epochs\n",
    "    epochs = 50\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \"\"\" Training \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        # forward pass\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            output = model(X_batch).squeeze()\n",
    "            y_preds = torch.round(output)\n",
    "\n",
    "            correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "            total += len(y_batch)\n",
    "\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            train_loss += loss.item()\n",
    "            # zero the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # backpropagattion\n",
    "            loss.backward()\n",
    "            # GD\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = (correct / total) * 100\n",
    "\n",
    "        \"\"\" Testing \"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                output = model(X_batch).squeeze()\n",
    "                y_preds = torch.round(output)\n",
    "\n",
    "                correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "                total += len(y_batch)\n",
    "\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                test_loss += loss.item()\n",
    "            \n",
    "            test_acc = (correct / total) * 100\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                torch.save(model.state_dict(), \"n-9best.pth\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f} | Acc: {train_acc:.2f}% | Learning Rate: {scheduler.get_last_lr()[0]:.7f} | Test loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold: 0\n",
      "Epoch: 0 | Train Loss: 76.91941 | Acc: 50.94% | Learning Rate: 0.0001000 | Test loss: 11.08252 | Test Acc: 55.01%\n",
      "Epoch: 10 | Train Loss: 19.83768 | Acc: 95.78% | Learning Rate: 0.0001000 | Test loss: 3.08297 | Test Acc: 95.60%\n",
      "Epoch: 20 | Train Loss: 18.06563 | Acc: 95.82% | Learning Rate: 0.0000100 | Test loss: 2.94070 | Test Acc: 95.65%\n",
      "Epoch: 30 | Train Loss: 17.67348 | Acc: 95.82% | Learning Rate: 0.0000100 | Test loss: 2.91258 | Test Acc: 95.65%\n",
      "Epoch: 40 | Train Loss: 17.48049 | Acc: 95.82% | Learning Rate: 0.0000010 | Test loss: 2.93551 | Test Acc: 95.65%\n",
      "\n",
      "\n",
      "Fold: 1\n",
      "Epoch: 0 | Train Loss: 76.95167 | Acc: 50.71% | Learning Rate: 0.0001000 | Test loss: 11.08535 | Test Acc: 51.65%\n",
      "Epoch: 10 | Train Loss: 20.43450 | Acc: 95.69% | Learning Rate: 0.0001000 | Test loss: 2.89520 | Test Acc: 95.85%\n",
      "Epoch: 20 | Train Loss: 18.30733 | Acc: 95.74% | Learning Rate: 0.0000100 | Test loss: 2.78704 | Test Acc: 95.90%\n",
      "Epoch: 30 | Train Loss: 18.07340 | Acc: 95.77% | Learning Rate: 0.0000100 | Test loss: 2.78959 | Test Acc: 95.90%\n",
      "Epoch: 40 | Train Loss: 17.93120 | Acc: 95.76% | Learning Rate: 0.0000010 | Test loss: 2.76731 | Test Acc: 95.90%\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "Epoch: 0 | Train Loss: 76.91881 | Acc: 50.89% | Learning Rate: 0.0001000 | Test loss: 11.08606 | Test Acc: 50.22%\n",
      "Epoch: 10 | Train Loss: 20.13766 | Acc: 95.74% | Learning Rate: 0.0001000 | Test loss: 2.92762 | Test Acc: 95.90%\n",
      "Epoch: 20 | Train Loss: 18.12243 | Acc: 95.78% | Learning Rate: 0.0000100 | Test loss: 2.86310 | Test Acc: 95.95%\n",
      "Epoch: 30 | Train Loss: 17.89978 | Acc: 95.79% | Learning Rate: 0.0000100 | Test loss: 2.87695 | Test Acc: 95.90%\n",
      "Epoch: 40 | Train Loss: 17.70760 | Acc: 95.80% | Learning Rate: 0.0000010 | Test loss: 2.87921 | Test Acc: 95.90%\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "Epoch: 0 | Train Loss: 76.93111 | Acc: 50.83% | Learning Rate: 0.0001000 | Test loss: 11.08090 | Test Acc: 55.01%\n",
      "Epoch: 10 | Train Loss: 20.89601 | Acc: 95.58% | Learning Rate: 0.0001000 | Test loss: 2.45317 | Test Acc: 96.79%\n",
      "Epoch: 20 | Train Loss: 18.77254 | Acc: 95.65% | Learning Rate: 0.0000100 | Test loss: 2.33245 | Test Acc: 96.79%\n",
      "Epoch: 30 | Train Loss: 18.45479 | Acc: 95.66% | Learning Rate: 0.0000100 | Test loss: 2.30204 | Test Acc: 96.79%\n",
      "Epoch: 40 | Train Loss: 18.31733 | Acc: 95.65% | Learning Rate: 0.0000010 | Test loss: 2.30326 | Test Acc: 96.79%\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "Epoch: 0 | Train Loss: 76.91452 | Acc: 50.81% | Learning Rate: 0.0001000 | Test loss: 11.08056 | Test Acc: 55.06%\n",
      "Epoch: 10 | Train Loss: 19.70587 | Acc: 95.77% | Learning Rate: 0.0001000 | Test loss: 3.18118 | Test Acc: 95.51%\n",
      "Epoch: 20 | Train Loss: 18.02493 | Acc: 95.78% | Learning Rate: 0.0000100 | Test loss: 3.08460 | Test Acc: 95.51%\n",
      "Epoch: 30 | Train Loss: 17.68825 | Acc: 95.81% | Learning Rate: 0.0000100 | Test loss: 3.12228 | Test Acc: 95.51%\n",
      "Epoch: 40 | Train Loss: 17.45704 | Acc: 95.82% | Learning Rate: 0.0000010 | Test loss: 3.13397 | Test Acc: 95.51%\n",
      "\n",
      "\n",
      "Fold: 5\n",
      "Epoch: 0 | Train Loss: 76.93796 | Acc: 50.41% | Learning Rate: 0.0001000 | Test loss: 11.08654 | Test Acc: 55.51%\n",
      "Epoch: 10 | Train Loss: 21.04012 | Acc: 95.70% | Learning Rate: 0.0001000 | Test loss: 3.56838 | Test Acc: 95.16%\n",
      "Epoch: 20 | Train Loss: 18.39873 | Acc: 95.82% | Learning Rate: 0.0000100 | Test loss: 3.26900 | Test Acc: 95.31%\n",
      "Epoch: 30 | Train Loss: 18.18561 | Acc: 95.82% | Learning Rate: 0.0000100 | Test loss: 3.27925 | Test Acc: 95.31%\n",
      "Epoch: 40 | Train Loss: 17.97054 | Acc: 95.82% | Learning Rate: 0.0000010 | Test loss: 3.27584 | Test Acc: 95.31%\n",
      "\n",
      "\n",
      "Fold: 6\n",
      "Epoch: 0 | Train Loss: 76.92967 | Acc: 51.21% | Learning Rate: 0.0001000 | Test loss: 11.08283 | Test Acc: 50.81%\n",
      "Epoch: 10 | Train Loss: 20.26757 | Acc: 95.75% | Learning Rate: 0.0001000 | Test loss: 3.15407 | Test Acc: 95.31%\n",
      "Epoch: 20 | Train Loss: 17.89816 | Acc: 95.80% | Learning Rate: 0.0000100 | Test loss: 2.91935 | Test Acc: 95.60%\n",
      "Epoch: 30 | Train Loss: 17.68463 | Acc: 95.81% | Learning Rate: 0.0000100 | Test loss: 2.93126 | Test Acc: 95.60%\n",
      "Epoch: 40 | Train Loss: 17.47898 | Acc: 95.80% | Learning Rate: 0.0000010 | Test loss: 2.93593 | Test Acc: 95.60%\n",
      "\n",
      "\n",
      "Fold: 7\n",
      "Epoch: 0 | Train Loss: 76.91837 | Acc: 50.79% | Learning Rate: 0.0001000 | Test loss: 11.08733 | Test Acc: 51.06%\n",
      "Epoch: 10 | Train Loss: 20.19601 | Acc: 95.68% | Learning Rate: 0.0001000 | Test loss: 3.24782 | Test Acc: 95.41%\n",
      "Epoch: 20 | Train Loss: 17.72700 | Acc: 95.82% | Learning Rate: 0.0000100 | Test loss: 3.18781 | Test Acc: 95.36%\n",
      "Epoch: 30 | Train Loss: 17.48697 | Acc: 95.83% | Learning Rate: 0.0000100 | Test loss: 3.22483 | Test Acc: 95.36%\n",
      "Epoch: 40 | Train Loss: 17.25098 | Acc: 95.84% | Learning Rate: 0.0000010 | Test loss: 3.23396 | Test Acc: 95.36%\n",
      "\n",
      "Average Accuracy is: 95.78395061728395\n"
     ]
    }
   ],
   "source": [
    "# Set up 8-fold cross-validation on the training dataset\n",
    "kf = KFold(n_splits=8, shuffle=True)\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "batch = 128\n",
    "accuracy = []\n",
    "# Generate train-test splits\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train_indices)):\n",
    "    print(\"\\n\\nFold:\", fold)\n",
    "    \"\"\" create dataloaders for each fold \"\"\"\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    test_subset = Subset(train_dataset, test_idx)\n",
    "    # Creating data loaders for each fold\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch, shuffle=False)\n",
    "\n",
    "    \"\"\" initialize the model, loss function, and optimizer \"\"\"\n",
    "    model = MLP(input_size=9, hidden_size=128, output_size=1)\n",
    "    loss_fn = nn.BCELoss()  # binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "    \"\"\" train and test the model \"\"\"\n",
    "    acc = train(model, train_loader, test_loader, loss_fn, optimizer, scheduler)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "print(\"\\nAverage Accuracy is:\", np.mean(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1279.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.67469 | Validation Acc: 96.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=9, hidden_size=128, output_size=1)\n",
    "model.load_state_dict(torch.load(\"n-9best.pth\"))\n",
    "model.eval()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0\n",
    "with torch.inference_mode():\n",
    "    for X_batch, y_batch in tqdm(val_loader):\n",
    "        output = model(X_batch).squeeze()\n",
    "        y_preds = torch.round(output)\n",
    "\n",
    "        correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    val_acc = (correct / total) * 100\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.5f} | Validation Acc: {val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
