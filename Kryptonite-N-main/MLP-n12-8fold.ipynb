{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 12)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load date\n",
    "X = np.load(\"Datasets/kryptonite-12-X.npy\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.load(\"Datasets/kryptonite-12-y.npy\")\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float)\n",
    "# create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# define split sizes (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# build model for training\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=12, hidden_size=128, output_size=1):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.layer3 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.layer4 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.layer5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = torch.sigmoid(self.layer5(x))  # sigmoid activation for binary output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, loss_fn, optimizer, scheduler):\n",
    "    # set the number of epochs\n",
    "    epochs = 50\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \"\"\" Training \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        # forward pass\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            output = model(X_batch).squeeze()\n",
    "            y_preds = torch.round(output)\n",
    "\n",
    "            correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "            total += len(y_batch)\n",
    "\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            train_loss += loss.item()\n",
    "            # zero the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # backpropagattion\n",
    "            loss.backward()\n",
    "            # GD\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = (correct / total) * 100\n",
    "\n",
    "        \"\"\" Testing \"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                output = model(X_batch).squeeze()\n",
    "                y_preds = torch.round(output)\n",
    "\n",
    "                correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "                total += len(y_batch)\n",
    "\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                test_loss += loss.item()\n",
    "            \n",
    "            test_acc = (correct / total) * 100\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                torch.save(model.state_dict(), \"n-12best.pth\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f} | Acc: {train_acc:.2f}% | Learning Rate: {scheduler.get_last_lr()[0]:.7f} | Test loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold: 0\n",
      "Epoch: 0 | Train Loss: 102.56178 | Acc: 50.90% | Learning Rate: 0.0001000 | Test loss: 15.25630 | Test Acc: 48.96%\n",
      "Epoch: 10 | Train Loss: 79.00825 | Acc: 77.51% | Learning Rate: 0.0001000 | Test loss: 10.79768 | Test Acc: 79.33%\n",
      "Epoch: 20 | Train Loss: 26.17272 | Acc: 95.74% | Learning Rate: 0.0001000 | Test loss: 4.44636 | Test Acc: 95.15%\n",
      "Epoch: 30 | Train Loss: 21.93110 | Acc: 96.20% | Learning Rate: 0.0000100 | Test loss: 4.14702 | Test Acc: 95.48%\n",
      "Epoch: 40 | Train Loss: 21.33052 | Acc: 96.23% | Learning Rate: 0.0000100 | Test loss: 4.17519 | Test Acc: 95.52%\n",
      "\n",
      "\n",
      "Fold: 1\n",
      "Epoch: 0 | Train Loss: 102.58627 | Acc: 50.75% | Learning Rate: 0.0001000 | Test loss: 15.24457 | Test Acc: 50.96%\n",
      "Epoch: 10 | Train Loss: 90.76874 | Acc: 69.90% | Learning Rate: 0.0001000 | Test loss: 12.58190 | Test Acc: 74.59%\n",
      "Epoch: 20 | Train Loss: 25.49173 | Acc: 95.80% | Learning Rate: 0.0001000 | Test loss: 3.88093 | Test Acc: 95.59%\n",
      "Epoch: 30 | Train Loss: 21.48999 | Acc: 96.25% | Learning Rate: 0.0000100 | Test loss: 3.54174 | Test Acc: 96.22%\n",
      "Epoch: 40 | Train Loss: 21.02661 | Acc: 96.28% | Learning Rate: 0.0000100 | Test loss: 3.52957 | Test Acc: 96.19%\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "Epoch: 0 | Train Loss: 102.58489 | Acc: 50.66% | Learning Rate: 0.0001000 | Test loss: 15.24976 | Test Acc: 50.67%\n",
      "Epoch: 10 | Train Loss: 65.53543 | Acc: 86.02% | Learning Rate: 0.0001000 | Test loss: 8.43041 | Test Acc: 90.04%\n",
      "Epoch: 20 | Train Loss: 25.21255 | Acc: 95.91% | Learning Rate: 0.0001000 | Test loss: 4.37715 | Test Acc: 95.33%\n",
      "Epoch: 30 | Train Loss: 21.77005 | Acc: 96.29% | Learning Rate: 0.0000100 | Test loss: 4.15120 | Test Acc: 95.78%\n",
      "Epoch: 40 | Train Loss: 21.40774 | Acc: 96.30% | Learning Rate: 0.0000100 | Test loss: 4.16442 | Test Acc: 95.59%\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "Epoch: 0 | Train Loss: 102.58294 | Acc: 50.40% | Learning Rate: 0.0001000 | Test loss: 15.24344 | Test Acc: 50.81%\n",
      "Epoch: 10 | Train Loss: 83.06250 | Acc: 73.95% | Learning Rate: 0.0001000 | Test loss: 11.25788 | Test Acc: 79.22%\n",
      "Epoch: 20 | Train Loss: 25.72992 | Acc: 95.58% | Learning Rate: 0.0001000 | Test loss: 4.51174 | Test Acc: 95.00%\n",
      "Epoch: 30 | Train Loss: 21.04100 | Acc: 96.30% | Learning Rate: 0.0000100 | Test loss: 4.33052 | Test Acc: 95.15%\n",
      "Epoch: 40 | Train Loss: 20.55083 | Acc: 96.38% | Learning Rate: 0.0000100 | Test loss: 4.36791 | Test Acc: 95.04%\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "Epoch: 0 | Train Loss: 102.61207 | Acc: 50.39% | Learning Rate: 0.0001000 | Test loss: 15.25412 | Test Acc: 51.11%\n",
      "Epoch: 10 | Train Loss: 66.35076 | Acc: 83.42% | Learning Rate: 0.0001000 | Test loss: 9.05294 | Test Acc: 85.70%\n",
      "Epoch: 20 | Train Loss: 27.38526 | Acc: 95.30% | Learning Rate: 0.0001000 | Test loss: 4.83381 | Test Acc: 93.70%\n",
      "Epoch: 30 | Train Loss: 23.16679 | Acc: 96.03% | Learning Rate: 0.0000100 | Test loss: 4.12930 | Test Acc: 95.30%\n",
      "Epoch: 40 | Train Loss: 22.54569 | Acc: 96.13% | Learning Rate: 0.0000100 | Test loss: 4.16402 | Test Acc: 95.33%\n",
      "\n",
      "\n",
      "Fold: 5\n",
      "Epoch: 0 | Train Loss: 102.57188 | Acc: 50.90% | Learning Rate: 0.0001000 | Test loss: 15.24880 | Test Acc: 50.48%\n",
      "Epoch: 10 | Train Loss: 55.24034 | Acc: 88.23% | Learning Rate: 0.0001000 | Test loss: 8.33360 | Test Acc: 84.19%\n",
      "Epoch: 20 | Train Loss: 27.00382 | Acc: 95.38% | Learning Rate: 0.0001000 | Test loss: 4.27704 | Test Acc: 94.74%\n",
      "Epoch: 30 | Train Loss: 22.93785 | Acc: 95.96% | Learning Rate: 0.0000100 | Test loss: 3.80847 | Test Acc: 95.81%\n",
      "Epoch: 40 | Train Loss: 22.47975 | Acc: 96.06% | Learning Rate: 0.0000100 | Test loss: 3.79402 | Test Acc: 95.67%\n",
      "\n",
      "\n",
      "Fold: 6\n",
      "Epoch: 0 | Train Loss: 102.57672 | Acc: 50.54% | Learning Rate: 0.0001000 | Test loss: 15.23380 | Test Acc: 52.63%\n",
      "Epoch: 10 | Train Loss: 82.22793 | Acc: 77.23% | Learning Rate: 0.0001000 | Test loss: 11.39854 | Test Acc: 78.56%\n",
      "Epoch: 20 | Train Loss: 27.24327 | Acc: 95.57% | Learning Rate: 0.0001000 | Test loss: 3.99888 | Test Acc: 95.67%\n",
      "Epoch: 30 | Train Loss: 22.72076 | Acc: 96.20% | Learning Rate: 0.0000100 | Test loss: 3.68187 | Test Acc: 96.04%\n",
      "Epoch: 40 | Train Loss: 22.26462 | Acc: 96.21% | Learning Rate: 0.0000100 | Test loss: 3.62741 | Test Acc: 96.22%\n",
      "\n",
      "\n",
      "Fold: 7\n",
      "Epoch: 0 | Train Loss: 102.59798 | Acc: 50.25% | Learning Rate: 0.0001000 | Test loss: 15.24344 | Test Acc: 51.19%\n",
      "Epoch: 10 | Train Loss: 63.83770 | Acc: 86.31% | Learning Rate: 0.0001000 | Test loss: 8.36681 | Test Acc: 89.74%\n",
      "Epoch: 20 | Train Loss: 26.23445 | Acc: 95.68% | Learning Rate: 0.0001000 | Test loss: 4.63879 | Test Acc: 94.89%\n",
      "Epoch: 30 | Train Loss: 22.45412 | Acc: 96.25% | Learning Rate: 0.0000100 | Test loss: 4.30588 | Test Acc: 95.33%\n",
      "Epoch: 40 | Train Loss: 21.98920 | Acc: 96.23% | Learning Rate: 0.0000100 | Test loss: 4.31523 | Test Acc: 95.33%\n",
      "\n",
      "Average Accuracy is: 95.76851851851852\n"
     ]
    }
   ],
   "source": [
    "# Set up 8-fold cross-validation on the training dataset\n",
    "kf = KFold(n_splits=8, shuffle=True)\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "batch = 128\n",
    "accuracy = []\n",
    "# Generate train-test splits\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train_indices)):\n",
    "    print(\"\\n\\nFold:\", fold)\n",
    "    \"\"\" create dataloaders for each fold \"\"\"\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    test_subset = Subset(train_dataset, test_idx)\n",
    "    # Creating data loaders for each fold\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch, shuffle=False)\n",
    "\n",
    "    \"\"\" initialize the model, loss function, and optimizer \"\"\"\n",
    "    model = MLP(input_size=12, hidden_size=128, output_size=1)\n",
    "    loss_fn = nn.BCELoss()  # binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    \"\"\" train and test the model \"\"\"\n",
    "    acc = train(model, train_loader, test_loader, loss_fn, optimizer, scheduler)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "print(\"\\nAverage Accuracy is:\", np.mean(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 1344.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.06801 | Validation Acc: 96.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=12, hidden_size=128, output_size=1)\n",
    "model.load_state_dict(torch.load(\"n-12best.pth\"))\n",
    "model.eval()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0\n",
    "with torch.inference_mode():\n",
    "    for X_batch, y_batch in tqdm(val_loader):\n",
    "        output = model(X_batch).squeeze()\n",
    "        y_preds = torch.round(output)\n",
    "\n",
    "        correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    val_acc = (correct / total) * 100\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.5f} | Validation Acc: {val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
