{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 15)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load date\n",
    "X = np.load(\"Datasets/kryptonite-15-X.npy\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.load(\"Datasets/kryptonite-15-y.npy\")\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float)\n",
    "# create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# define split sizes (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# build model for training\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=15, hidden_size=128, output_size=1):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.layer3 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.layer4 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.layer5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = torch.sigmoid(self.layer5(x))  # sigmoid activation for binary output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, loss_fn, optimizer, scheduler):\n",
    "    torch.manual_seed(42)\n",
    "    # set the number of epochs\n",
    "    epochs = 60\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \"\"\" Training \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        # forward pass\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            output = model(X_batch).squeeze()\n",
    "            y_preds = torch.round(output)\n",
    "\n",
    "            correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "            total += len(y_batch)\n",
    "\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            train_loss += loss.item()\n",
    "            # zero the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # backpropagattion\n",
    "            loss.backward()\n",
    "            # GD\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = (correct / total) * 100\n",
    "\n",
    "        \"\"\" Testing \"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                output = model(X_batch).squeeze()\n",
    "                y_preds = torch.round(output)\n",
    "\n",
    "                correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "                total += len(y_batch)\n",
    "\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                test_loss += loss.item()\n",
    "            \n",
    "            test_acc = (correct / total) * 100\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                torch.save(model.state_dict(), \"n-15best.pth\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f} | Acc: {train_acc:.2f}% | Learning Rate: {scheduler.get_last_lr()[0]:.7f} | Test loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold: 0\n",
      "Epoch: 0 | Train Loss: 128.24710 | Acc: 49.83% | Learning Rate: 0.0001000 | Test loss: 18.71613 | Test Acc: 49.78%\n",
      "Epoch: 10 | Train Loss: 127.65158 | Acc: 53.73% | Learning Rate: 0.0001000 | Test loss: 18.75695 | Test Acc: 49.93%\n",
      "Epoch: 20 | Train Loss: 59.88523 | Acc: 87.62% | Learning Rate: 0.0001000 | Test loss: 9.32254 | Test Acc: 85.63%\n",
      "Epoch: 30 | Train Loss: 39.09099 | Acc: 93.36% | Learning Rate: 0.0001000 | Test loss: 8.56876 | Test Acc: 87.53%\n",
      "Epoch: 40 | Train Loss: 29.65409 | Acc: 95.63% | Learning Rate: 0.0000100 | Test loss: 5.91241 | Test Acc: 93.24%\n",
      "Epoch: 50 | Train Loss: 28.19487 | Acc: 95.87% | Learning Rate: 0.0000100 | Test loss: 6.06411 | Test Acc: 92.89%\n",
      "\n",
      "\n",
      "Fold: 1\n",
      "Epoch: 0 | Train Loss: 128.23777 | Acc: 49.99% | Learning Rate: 0.0001000 | Test loss: 18.71546 | Test Acc: 49.93%\n",
      "Epoch: 10 | Train Loss: 127.96831 | Acc: 52.28% | Learning Rate: 0.0001000 | Test loss: 18.73369 | Test Acc: 49.36%\n",
      "Epoch: 20 | Train Loss: 101.90209 | Acc: 71.82% | Learning Rate: 0.0001000 | Test loss: 15.61134 | Test Acc: 69.21%\n",
      "Epoch: 30 | Train Loss: 44.29824 | Acc: 91.91% | Learning Rate: 0.0001000 | Test loss: 8.20483 | Test Acc: 89.19%\n",
      "Epoch: 40 | Train Loss: 28.24174 | Acc: 95.71% | Learning Rate: 0.0000100 | Test loss: 6.56865 | Test Acc: 92.36%\n",
      "Epoch: 50 | Train Loss: 26.35687 | Acc: 96.13% | Learning Rate: 0.0000100 | Test loss: 6.65045 | Test Acc: 92.36%\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "Epoch: 0 | Train Loss: 128.23954 | Acc: 49.77% | Learning Rate: 0.0001000 | Test loss: 18.71553 | Test Acc: 49.75%\n",
      "Epoch: 10 | Train Loss: 128.00308 | Acc: 52.21% | Learning Rate: 0.0001000 | Test loss: 18.71333 | Test Acc: 50.81%\n",
      "Epoch: 20 | Train Loss: 74.73350 | Acc: 83.07% | Learning Rate: 0.0001000 | Test loss: 11.41169 | Test Acc: 81.19%\n",
      "Epoch: 30 | Train Loss: 42.54705 | Acc: 92.06% | Learning Rate: 0.0001000 | Test loss: 8.84052 | Test Acc: 87.50%\n",
      "Epoch: 40 | Train Loss: 31.14132 | Acc: 94.99% | Learning Rate: 0.0000100 | Test loss: 6.91932 | Test Acc: 91.88%\n",
      "Epoch: 50 | Train Loss: 29.69541 | Acc: 95.31% | Learning Rate: 0.0000100 | Test loss: 6.94524 | Test Acc: 91.91%\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "Epoch: 0 | Train Loss: 128.24191 | Acc: 49.89% | Learning Rate: 0.0001000 | Test loss: 18.71453 | Test Acc: 50.43%\n",
      "Epoch: 10 | Train Loss: 128.03379 | Acc: 51.83% | Learning Rate: 0.0001000 | Test loss: 18.72320 | Test Acc: 50.81%\n",
      "Epoch: 20 | Train Loss: 112.09919 | Acc: 68.01% | Learning Rate: 0.0001000 | Test loss: 17.31504 | Test Acc: 63.23%\n",
      "Epoch: 30 | Train Loss: 53.98590 | Acc: 88.66% | Learning Rate: 0.0001000 | Test loss: 10.29564 | Test Acc: 84.56%\n",
      "Epoch: 40 | Train Loss: 34.25792 | Acc: 94.20% | Learning Rate: 0.0000100 | Test loss: 7.80991 | Test Acc: 90.28%\n",
      "Epoch: 50 | Train Loss: 32.11415 | Acc: 94.73% | Learning Rate: 0.0000100 | Test loss: 7.86308 | Test Acc: 90.70%\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "Epoch: 0 | Train Loss: 128.23826 | Acc: 50.18% | Learning Rate: 0.0001000 | Test loss: 18.71431 | Test Acc: 50.49%\n",
      "Epoch: 10 | Train Loss: 128.02180 | Acc: 52.25% | Learning Rate: 0.0001000 | Test loss: 18.74269 | Test Acc: 49.16%\n",
      "Epoch: 20 | Train Loss: 106.00825 | Acc: 70.81% | Learning Rate: 0.0001000 | Test loss: 15.76437 | Test Acc: 69.13%\n",
      "Epoch: 30 | Train Loss: 42.12257 | Acc: 92.29% | Learning Rate: 0.0001000 | Test loss: 8.51807 | Test Acc: 88.68%\n",
      "Epoch: 40 | Train Loss: 27.02736 | Acc: 95.91% | Learning Rate: 0.0000100 | Test loss: 6.37999 | Test Acc: 92.80%\n",
      "Epoch: 50 | Train Loss: 25.18542 | Acc: 96.19% | Learning Rate: 0.0000100 | Test loss: 6.46769 | Test Acc: 92.95%\n",
      "\n",
      "\n",
      "Fold: 5\n",
      "Epoch: 0 | Train Loss: 128.24645 | Acc: 49.20% | Learning Rate: 0.0001000 | Test loss: 18.71437 | Test Acc: 51.97%\n",
      "Epoch: 10 | Train Loss: 128.03874 | Acc: 52.08% | Learning Rate: 0.0001000 | Test loss: 18.71733 | Test Acc: 50.04%\n",
      "Epoch: 20 | Train Loss: 63.57709 | Acc: 86.62% | Learning Rate: 0.0001000 | Test loss: 9.75896 | Test Acc: 85.16%\n",
      "Epoch: 30 | Train Loss: 33.96000 | Acc: 94.56% | Learning Rate: 0.0001000 | Test loss: 6.17105 | Test Acc: 92.92%\n",
      "Epoch: 40 | Train Loss: 26.06541 | Acc: 96.32% | Learning Rate: 0.0000100 | Test loss: 5.65914 | Test Acc: 93.93%\n",
      "Epoch: 50 | Train Loss: 25.04266 | Acc: 96.43% | Learning Rate: 0.0000100 | Test loss: 5.83659 | Test Acc: 93.45%\n",
      "\n",
      "\n",
      "Fold: 6\n",
      "Epoch: 0 | Train Loss: 128.24271 | Acc: 49.84% | Learning Rate: 0.0001000 | Test loss: 18.71478 | Test Acc: 50.19%\n",
      "Epoch: 10 | Train Loss: 128.08763 | Acc: 51.64% | Learning Rate: 0.0001000 | Test loss: 18.71792 | Test Acc: 51.23%\n",
      "Epoch: 20 | Train Loss: 62.26598 | Acc: 86.81% | Learning Rate: 0.0001000 | Test loss: 8.86689 | Test Acc: 86.87%\n",
      "Epoch: 30 | Train Loss: 38.24552 | Acc: 93.48% | Learning Rate: 0.0001000 | Test loss: 5.90302 | Test Acc: 92.68%\n",
      "Epoch: 40 | Train Loss: 28.95771 | Acc: 95.64% | Learning Rate: 0.0000100 | Test loss: 5.37190 | Test Acc: 93.66%\n",
      "Epoch: 50 | Train Loss: 27.65216 | Acc: 95.91% | Learning Rate: 0.0000100 | Test loss: 5.38868 | Test Acc: 93.69%\n",
      "\n",
      "\n",
      "Fold: 7\n",
      "Epoch: 0 | Train Loss: 128.24054 | Acc: 49.92% | Learning Rate: 0.0001000 | Test loss: 18.71618 | Test Acc: 48.62%\n",
      "Epoch: 10 | Train Loss: 127.91746 | Acc: 52.58% | Learning Rate: 0.0001000 | Test loss: 18.74742 | Test Acc: 50.01%\n",
      "Epoch: 20 | Train Loss: 82.41542 | Acc: 79.89% | Learning Rate: 0.0001000 | Test loss: 12.35192 | Test Acc: 78.81%\n",
      "Epoch: 30 | Train Loss: 39.33872 | Acc: 92.75% | Learning Rate: 0.0001000 | Test loss: 6.87013 | Test Acc: 91.38%\n",
      "Epoch: 40 | Train Loss: 27.37121 | Acc: 95.86% | Learning Rate: 0.0000100 | Test loss: 6.04317 | Test Acc: 92.92%\n",
      "Epoch: 50 | Train Loss: 25.94569 | Acc: 96.09% | Learning Rate: 0.0000100 | Test loss: 6.01952 | Test Acc: 93.21%\n",
      "\n",
      "Average Accuracy is: 92.91481481481482\n"
     ]
    }
   ],
   "source": [
    "# Set up 8-fold cross-validation on the training dataset\n",
    "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "batch = 128\n",
    "accuracy = []\n",
    "# Generate train-test splits\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train_indices)):\n",
    "    print(\"\\n\\nFold:\", fold)\n",
    "    \"\"\" create dataloaders for each fold \"\"\"\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    test_subset = Subset(train_dataset, test_idx)\n",
    "    # Creating data loaders for each fold\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch, shuffle=False)\n",
    "\n",
    "    \"\"\" initialize the model, loss function, and optimizer \"\"\"\n",
    "    model = MLP(input_size=15, hidden_size=128, output_size=1)\n",
    "    loss_fn = nn.BCELoss()  # binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "    \"\"\" train and test the model \"\"\"\n",
    "    acc = train(model, train_loader, test_loader, loss_fn, optimizer, scheduler)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "print(\"\\nAverage Accuracy is:\", np.mean(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 1405.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.02529 | Validation Acc: 92.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=15, hidden_size=128, output_size=1)\n",
    "model.load_state_dict(torch.load(\"n-15best.pth\"))\n",
    "model.eval()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0\n",
    "with torch.inference_mode():\n",
    "    for X_batch, y_batch in tqdm(val_loader):\n",
    "        output = model(X_batch).squeeze()\n",
    "        y_preds = torch.round(output)\n",
    "\n",
    "        correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    val_acc = (correct / total) * 100\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.5f} | Validation Acc: {val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
