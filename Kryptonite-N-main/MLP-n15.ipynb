{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 15)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load date\n",
    "X = np.load(\"Datasets/kryptonite-15-X.npy\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.load(\"Datasets/kryptonite-15-y.npy\")\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 15]) torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# define split sizes (70% train, 15% validation, 15% test)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each subset\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# check data loader output\n",
    "for X_batch, y_batch in tqdm(train_loader):\n",
    "    print(X_batch.shape, y_batch.shape)  # X_batch is of shape [batch_size, *input_shape]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# build model for training\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=15, hidden_size=128, output_size=1):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.layer3 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.layer4 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.layer5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = torch.sigmoid(self.layer5(x))  # sigmoid activation for binary output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model, loss function, and optimizer\n",
    "model = MLP(input_size=15, hidden_size=128, output_size=1)\n",
    "loss_fn = nn.BCELoss()  # binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 114.38105 | Acc: 50.27% | Learning Rate: 0.0001000 | Test loss: 24.95025 | Test Acc: 50.96%\n",
      "Epoch: 10 | Train Loss: 114.16007 | Acc: 52.02% | Learning Rate: 0.0001000 | Test loss: 24.95427 | Test Acc: 49.96%\n",
      "Epoch: 20 | Train Loss: 52.22822 | Acc: 88.21% | Learning Rate: 0.0001000 | Test loss: 12.18556 | Test Acc: 87.47%\n",
      "Epoch: 30 | Train Loss: 36.82878 | Acc: 92.94% | Learning Rate: 0.0001000 | Test loss: 10.76105 | Test Acc: 89.47%\n",
      "Epoch: 40 | Train Loss: 28.37321 | Acc: 95.05% | Learning Rate: 0.0000100 | Test loss: 9.41182 | Test Acc: 91.84%\n",
      "Epoch: 50 | Train Loss: 27.38853 | Acc: 95.30% | Learning Rate: 0.0000100 | Test loss: 9.53573 | Test Acc: 91.98%\n",
      "Epoch: 60 | Train Loss: 26.50051 | Acc: 95.56% | Learning Rate: 0.0000100 | Test loss: 9.59674 | Test Acc: 91.98%\n",
      "Epoch: 70 | Train Loss: 25.96151 | Acc: 95.62% | Learning Rate: 0.0000100 | Test loss: 9.70565 | Test Acc: 91.91%\n",
      "Epoch: 80 | Train Loss: 25.43925 | Acc: 95.78% | Learning Rate: 0.0000010 | Test loss: 9.74136 | Test Acc: 91.91%\n",
      "Epoch: 90 | Train Loss: 24.87245 | Acc: 95.84% | Learning Rate: 0.0000010 | Test loss: 9.73370 | Test Acc: 91.93%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# set the number of epochs\n",
    "epochs = 100\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \"\"\" Training \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # forward pass\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        output = model(X_batch).squeeze()\n",
    "        y_preds = torch.round(output)\n",
    "\n",
    "        correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        # zero the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # backpropagattion\n",
    "        loss.backward()\n",
    "        # GD\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_acc = (correct / total) * 100\n",
    "\n",
    "    \"\"\" Testing \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch).squeeze()\n",
    "            y_preds = torch.round(output)\n",
    "\n",
    "            correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "            total += len(y_batch)\n",
    "\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "        test_acc = (correct / total) * 100\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), \"n-15best.pth\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f} | Acc: {train_acc:.2f}% | Learning Rate: {scheduler.get_last_lr()[0]:.7f} | Test loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 1615.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.57605 | Validation Acc: 92.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=15, hidden_size=128, output_size=1)\n",
    "model.load_state_dict(torch.load(\"n-15best.pth\"))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0\n",
    "with torch.inference_mode():\n",
    "    for X_batch, y_batch in tqdm(val_loader):\n",
    "        output = model(X_batch).squeeze()\n",
    "        y_preds = torch.round(output)\n",
    "\n",
    "        correct += torch.eq(y_preds, y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    val_acc = (correct / total) * 100\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.5f} | Validation Acc: {val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
